# ğŸ¯ Real-Time Facial Expression & Engagement Detection System

A Deep Learning-based system that analyzes video input to:

* ğŸ” Detect **facial expressions (frame-by-frame)**
* ğŸ“Š Predict **overall engagement level (LOW / MEDIUM / HIGH)**
* ğŸ¥ Generate an output video with real-time overlays

This project combines **OpenFace**, **Random Forest**, and **BiLSTM neural networks** to create a full video-based engagement analysis pipeline.

---

## ğŸš€ Features

âœ… Automatic OpenFace feature extraction
âœ… Frame-level facial expression prediction
âœ… Video-level engagement classification
âœ… Real-time overlay on output video
âœ… Handles new unseen videos
âœ… Fully modular training & inference pipeline

---

## ğŸ—ï¸ Architecture Overview

### 1ï¸âƒ£ Feature Extraction

* Tool: **OpenFace**
* Extracts:

  * Head pose (p_rx, p_ry, p_rz)
  * Action Units (AU01_r, AU02_r, â€¦)
  * Facial landmarks

### 2ï¸âƒ£ Expression Model

* Algorithm: **Random Forest**
* Input: OpenFace AU + head pose features
* Output: Expression class (per frame)

### 3ï¸âƒ£ Engagement Model

* Algorithm: **BiLSTM + LSTM**
* Input: Sequential OpenFace features (+ optional expression fusion)
* Output:

  * LOW
  * MEDIUM
  * HIGH

---

## ğŸ§  Model Architecture

### Expression Model

```
OpenFace Features â†’ StandardScaler â†’ RandomForest â†’ Expression Label
```

### Engagement Model

```
Sequential Features (per video)
        â†“
Masking Layer
        â†“
Bidirectional LSTM (64 units)
        â†“
LSTM (32 units)
        â†“
Dense + Dropout
        â†“
Softmax â†’ Engagement Level
```

---

## ğŸ“‚ Project Structure

```
EngagementDetectionProject/
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ master_dataset.csv
â”‚   â””â”€â”€ labeled_dataset.csv
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ expression_model.pkl
â”‚   â”œâ”€â”€ expression_scaler.pkl
â”‚   â”œâ”€â”€ engagement_model_lstm_rnn.h5
â”‚   â””â”€â”€ engagement_scaler.pkl
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_expression_model.py
â”‚   â”œâ”€â”€ train_model.py
â”‚   â””â”€â”€ predict_final_video.py
â”‚
â”œâ”€â”€ OpenFace/
â”‚
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/your-repo-name.git
cd your-repo-name
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv engagement_env
engagement_env\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

Required libraries:

* tensorflow
* scikit-learn
* pandas
* numpy
* opencv-python
* joblib

---

## ğŸ‹ï¸ Training

### Train Expression Model

```bash
python train_expression_model.py
```

This will generate:

```
models/expression_model.pkl
models/expression_scaler.pkl
models/expression_label_encoder.pkl
```

---

### Train Engagement Model

```bash
python train_model.py
```

This will generate:

```
models/engagement_model_lstm_rnn.h5
models/engagement_scaler.pkl
models/engagement_label_encoder.pkl
```

---

## ğŸ¥ Run Prediction on New Video

```bash
python predict_final_video.py --video "../input/test_video.mp4"
```

### Output:

* Extracts features using OpenFace
* Predicts frame-level expressions
* Predicts overall engagement
* Saves annotated video:

```
test_video_output.mp4
```

Overlay Example:

```
Expression: Angry
Engagement: LOW
```

---

## ğŸ“Š Engagement Levels

| Class | Meaning |
| ----- | ------- |
| 0     | LOW     |
| 1     | MEDIUM  |
| 2     | HIGH    |

---

## ğŸ”¬ Dataset Description

The dataset contains:

* Video ID
* Frame-level OpenFace features
* Expression labels
* Video-level engagement label

Expression is predicted per frame.
Engagement is predicted per video sequence.

---

## ğŸ’¡ Future Improvements

* ğŸ”„ Real-time webcam support
* ğŸ“ˆ Attention mechanism in LSTM
* ğŸŒ Web deployment (Streamlit / Flask)
* ğŸ“Š Temporal smoothing for engagement

---

## ğŸ“ Applications

* Online learning engagement monitoring
* Classroom attention analysis
* Behavioral research
* Human-computer interaction studies
* Interview performance analytics

---

## ğŸ‘¨â€ğŸ’» Author

Developed as part of a deep learning research project on video-based engagement estimation.

